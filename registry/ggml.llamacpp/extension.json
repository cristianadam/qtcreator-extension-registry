{
    "$schema": "../../schema/extension.schema.json",
    "info": {
        "id": "llamacpp",
        "vendor_id": "ggml",
        "display_name": "llama.qtcreator",
        "display_vendor": "ggml-org",
        "license": "open-source"
    },
    "latest": "2.0.0",
    "versions": {
        "2.0.0": {
            "sources": [
                {
                    "url": "https://github.com/cristianadam/llama.qtcreator/releases/download/v2.0.0/LlamaCpp-2.0.0-Windows-x64.7z",
                    "sha256": "1be7fd277df56e1757e3936509c4ef17245c1d4d12bfc5a938f4140dc89c59e3",
                    "platform": {
                        "name": "Windows",
                        "architecture": "x86_64"
                    }
                },
                {
                    "url": "https://github.com/cristianadam/llama.qtcreator/releases/download/v2.0.0/LlamaCpp-2.0.0-Windows-arm64.7z",
                    "sha256": "60a01a573b31e15628046cf67af59d8ed3ed3cfc831d803cad10e9a98ec61676",
                    "platform": {
                        "name": "Windows",
                        "architecture": "arm64"
                    }
                },
                {
                    "url": "https://github.com/cristianadam/llama.qtcreator/releases/download/v2.0.0/LlamaCpp-2.0.0-Linux-x64.7z",
                    "sha256": "9d6cd83767fd8b5ccf8fac3c2eeb4e3dc8ead4fd382e2abbe6e2a688c1e33977",
                    "platform": {
                        "name": "Linux",
                        "architecture": "x86_64"
                    }
                },
                {
                    "url": "https://github.com/cristianadam/llama.qtcreator/releases/download/v2.0.0/LlamaCpp-2.0.0-Linux-arm64.7z",
                    "sha256": "bed301ef01ad4e23b5bbf1f7d8d76310dc8712d6b189f236f1f183d5bf2ea798",
                    "platform": {
                        "name": "Linux",
                        "architecture": "arm64"
                    }
                },
                {
                    "url": "https://github.com/cristianadam/llama.qtcreator/releases/download/v2.0.0/LlamaCpp-2.0.0-macOS-universal.7z",
                    "sha256": "03303dabf0be60483451d5d1f1aa1d2c40400f99294dc3aa57ff9fa4030b2cf8",
                    "platform": {
                        "name": "macOS",
                        "architecture": "x86_64"
                    }
                },
                {
                    "url": "https://github.com/cristianadam/llama.qtcreator/releases/download/v2.0.0/LlamaCpp-2.0.0-macOS-universal.7z",
                    "sha256": "03303dabf0be60483451d5d1f1aa1d2c40400f99294dc3aa57ff9fa4030b2cf8",
                    "platform": {
                        "name": "macOS",
                        "architecture": "arm64"
                    }
                }
            ],
            "metadata": {
                "Id": "llamacpp",
                "Name": "llama.qtcreator",
                "Version": "2.0.0",
                "CompatVersion": "2.0.0",
                "Vendor": "ggml-org",
                "VendorId": "ggml",
                "Copyright": "(C) 2025 The llama.qtcreator Contributors, Copyright (C) The Qt Company Ltd. and other contributors.",
                "License": "MIT",
                "Description": "llama.cpp infill completion plugin for Qt Creator",
                "LongDescription": [
                    "# llama.qtcreator",
                    "",
                    "Local LLM-assisted text completion for Qt Creator.",
                    "",
                    "![Qt Creator llama.cpp Text](https://raw.githubusercontent.com/cristianadam/llama.qtcreator/refs/heads/main/screenshots/qtcreator-llamacpp-text@2x.webp)",
                    "",
                    "---",
                    "",
                    "![Qt Creator llama.cpp Qt Widgets](https://raw.githubusercontent.com/cristianadam/llama.qtcreator/refs/heads/main/screenshots/qtcreator-llamacpp-widgets@2x.webp)",
                    "",
                    "",
                    "## Features",
                    "",
                    "- Auto-suggest on cursor movement. Toggle enable / disable with `Ctrl+Shift+G`",
                    "- Trigger the suggestion manually by pressing `Ctrl+G`",
                    "- Accept a suggestion with `Tab`",
                    "- Accept the first line of a suggestion with `Shift+Tab`",
                    "- Control max text generation time",
                    "- Configure scope of context around the cursor",
                    "- Ring context with chunks from open and edited files and yanked text",
                    "- [Supports very large contexts even on low-end hardware via smart context reuse](https://github.com/ggml-org/llama.cpp/pull/9787)",
                    "- Speculative FIM support",
                    "- Speculative Decoding support",
                    "- Display performance stats",
                    "- Chat support",
                    "- Source and Image drag & drop support",
                    "- Current editor selection predefined and custom LLM prompts",
                    "",
                    "",
                    "### llama.cpp setup",
                    "",
                    "The plugin requires a [llama.cpp](https://github.com/ggml-org/llama.cpp) server instance to be running at:",
                    "",
                    "![Qt Creator llama.cpp Settings](https://raw.githubusercontent.com/cristianadam/llama.qtcreator/refs/heads/main/screenshots/qtcreator-llamacpp-settings@2x.webp)",
                    "",
                    "",
                    "#### Mac OS",
                    "",
                    "```bash",
                    "brew install llama.cpp",
                    "```",
                    "",
                    "#### Windows",
                    "",
                    "```bash",
                    "winget install llama.cpp",
                    "```",
                    "",
                    "#### Any other OS",
                    "",
                    "Either build from source or use the latest binaries: https://github.com/ggml-org/llama.cpp/releases",
                    "",
                    "### llama.cpp settings",
                    "",
                    "Here are recommended settings, depending on the amount of VRAM that you have:",
                    "",
                    "- More than 16GB VRAM:",
                    "",
                    "  ```bash",
                    "  llama-server --fim-qwen-7b-default",
                    "  ```",
                    "",
                    "- Less than 16GB VRAM:",
                    "",
                    "  ```bash",
                    "  llama-server --fim-qwen-3b-default",
                    "  ```",
                    "",
                    "- Less than 8GB VRAM:",
                    "",
                    "  ```bash",
                    "  llama-server --fim-qwen-1.5b-default",
                    "  ```",
                    "",
                    "Use `llama-server --help` for more details.",
                    "",
                    "",
                    "### Recommended LLMs",
                    "",
                    "The plugin requires FIM-compatible models: [HF collection](https://huggingface.co/collections/ggml-org/llamavim-6720fece33898ac10544ecf9)",
                    "",
                    "## Examples",
                    "",
                    "### A Qt Quick example on MacBook Pro M3 `Qwen2.5-Coder 3B Q8_0`:",
                    "",
                    "![Qt Creator llama.cpp Qt Quick](https://raw.githubusercontent.com/cristianadam/llama.qtcreator/refs/heads/main/screenshots/qtcreator-llamacpp-quick@2x.webp)",
                    "",
                    "### Chat on a Mac Studio M2 with `gpt-oss 20B`:",
                    "",
                    "![Qt Creator llama.cpp Chat](https://raw.githubusercontent.com/cristianadam/llama.qtcreator/refs/heads/main/screenshots/qtcreator-llamacpp-chat.webp)",
                    "",
                    "## Implementation details",
                    "",
                    "The plugin aims to be very simple and lightweight and at the same time to provide high-quality and performant local FIM completions, even on consumer-grade hardware. ",
                    "",
                    "## Other IDEs",
                    "",
                    "- Vim/Neovim: https://github.com/ggml-org/llama.vim",
                    "- VS Code: https://github.com/ggml-org/llama.vscode"
                ],
                "Url": "https://github.com/ggml-org/llama.qtcreator",
                "DocumentationUrl": "",
                "Dependencies": [
                    {
                        "Id": "core",
                        "Version": "17.0.1"
                    },
                    {
                        "Id": "projectexplorer",
                        "Version": "17.0.1"
                    },
                    {
                        "Id": "texteditor",
                        "Version": "17.0.1"
                    }
                ]
            }
        }
    }
}